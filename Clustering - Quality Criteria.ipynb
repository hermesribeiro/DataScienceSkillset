{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "In this series, we intend to provide implementations of the methods and algorithms described by [Rokach and Maimon 2005, ch 15](http://www.springer.com/us/book/9780387254654). Although these routines can be found on available packages like SciPy, the routines below are for educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal quality criteria\n",
    "Internal quality metrics usually measure the compactness of the clusters using some similarity measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Squared Errors\n",
    "SSE is the simplest and most\n",
    "widely used criterion measure for clustering. It is calculated as:\n",
    "$$SSE = \\sum_{k=1}^K \\sum_{\\forall x_i \\in C_k} \\|x_i-\\mu_k\\|^2$$\n",
    "where $C_k$ is the set of instances in cluster k; $\\mu_k$ is the vector mean of cluster\n",
    "k. The components of $\\mu_k$ are calculated as:\n",
    "$$\\mu_k=\\frac{1}{N_k} \\sum_{\\forall x_i  \\in C_k} x_{i,k}$$\n",
    "Clustering methods that minimize the SSE criterion are often called minimum variance partitions, since by simple algebraic manipulation the SSE criterion may be written as:\n",
    "$$SSE = \\frac{1}{2} \\sum_{k=1}^K N_k \\bar S_k $$\n",
    "Where:\n",
    "$$\\bar S_k=\\frac{1}{n_k^2} \\sum_{x_i,x_j \\in C_k} \\|x_i-x_j\\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will establish a toy example to implement our quality criteria on. We will use the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'target', 'target_names']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance as spd\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "dir(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we implement standard $SSE$ calculation routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabs(data,target):\n",
    "    # Listing unique labels\n",
    "    labs = []\n",
    "    for idx in range(len(target)):\n",
    "        if not (target[idx] in labs):\n",
    "            labs.append(target[idx])\n",
    "    return labs\n",
    "\n",
    "def getPosits(data,target):\n",
    "    labs = getLabs(data,target)\n",
    "    posits = []\n",
    "    # Retrieving target positions\n",
    "    for idy in range(len(labs)):\n",
    "        temp=[]\n",
    "        for idx in range(len(target)):\n",
    "            if target[idx] == labs[idy]:\n",
    "                temp.append(idx)\n",
    "        posits.append(temp)\n",
    "    return posits\n",
    "    \n",
    "def stdSSE(data,target):\n",
    "    # Cluster means\n",
    "    labs = getLabs(data,target)\n",
    "    posits = getPosits(data,target)\n",
    "    means = np.empty([len(labs),len(data[0])])\n",
    "    for idy in range(len(labs)): means[idy] = np.mean(data[posits[idy]],axis=0)\n",
    "    # SSE\n",
    "    SSE=0\n",
    "    for idy in range(len(labs)):\n",
    "        for idx in posits[idy]:\n",
    "            SSE = SSE + spd.euclidean(data[idx],means[idy])**2\n",
    "    return SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.38680000000004"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdSSE(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Variance\n",
    "We can now define the minimum variance method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minVar(data,target,dissimilarity):\n",
    "    # Listing unique targets\n",
    "    labs = getLabs(data,target)\n",
    "    posits = getPosits(data,target)\n",
    "    S = np.empty(len(labs))\n",
    "    for idy in range(len(labs)):\n",
    "        temp = 0\n",
    "        for idxi in range(len(posits[idy])):\n",
    "            for idxj in range(len(posits[idy])):\n",
    "                temp = temp + dissimilarity(data[posits[idy][idxi]],data[posits[idy][idxj]])\n",
    "        S[idy] = temp/(len(posits[idy])**2)\n",
    "    SSE = 0\n",
    "    for idy in range(len(labs)): SSE = SSE + float(len(posits[idy]))*S[idy]\n",
    "    SSE = SSE/2\n",
    "    return SSE\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Them, for the euclidean distance (the classic form of SSE), one should provide the dataset and the squared euclidean function as argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.386800000000022"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quadEuclidean(a,b):\n",
    "    res = spd.euclidean(a,b)**2\n",
    "    return res\n",
    "minVar(iris.data,iris.target,quadEuclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this construction is the possibility to use other similarity methods. A disadvantage is that while the direct construction is O(N), the minimum variance construction is of O(N^2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manhattan minimum variance =  118.488\n",
      "euclidean minimum variance =  70.370453723\n",
      "Chebyshev minimum variance =  54.326\n",
      "Hamming minimum variance =  65.4\n",
      "Cosine minimum variance =  0.168183278276\n",
      "Pearson minimum variance =  0.480793106828\n"
     ]
    }
   ],
   "source": [
    "print(\"manhattan minimum variance = \",minVar(iris.data,iris.target,spd.cityblock))\n",
    "print(\"euclidean minimum variance = \", minVar(iris.data,iris.target,spd.euclidean))\n",
    "print(\"Chebyshev minimum variance = \", minVar(iris.data,iris.target,spd.chebyshev))\n",
    "print(\"Hamming minimum variance = \", minVar(iris.data,iris.target,spd.hamming))\n",
    "print(\"Cosine minimum variance = \",minVar(iris.data,iris.target,spd.cosine))\n",
    "print(\"Pearson minimum variance = \",minVar(iris.data,iris.target,spd.correlation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Criteria\n",
    "The scatter criteria can be the within cluster criterion $S_B$, between cluster criterion $S_B$, total cluster criterion $S_T$ or a combination of them.\n",
    "$S_W$ can be defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sWithin (data,target):\n",
    "    labs = getLabs(data,target)\n",
    "    posits = getPosits(data,target)\n",
    "    Sk = np.zeros([len(data[0]),len(data[0])])\n",
    "    for idy in range(len(labs)):\n",
    "        cmean = np.mean(data[posits[idy]],axis=0)\n",
    "        for vec in data[posits[idy]]:\n",
    "            Sk = Sk + np.outer(vec-cmean,vec-cmean)\n",
    "    return Sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 38.9562,  13.683 ,  24.614 ,   5.6556],\n",
       "       [ 13.683 ,  17.035 ,   8.12  ,   4.9132],\n",
       "       [ 24.614 ,   8.12  ,  27.22  ,   6.2536],\n",
       "       [  5.6556,   4.9132,   6.2536,   6.1756]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sWithin(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And $S_B$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sBetween (data,target):\n",
    "    labs = getLabs(data,target)\n",
    "    posits = getPosits(data,target)\n",
    "    Sb = np.zeros([len(data[0]),len(data[0])])\n",
    "    cmean = 0\n",
    "    tmean = np.zeros([len(data[0])])\n",
    "    # Calculating the average of the cluster centers (which is equal to averaging over the whole dataset ¯\\_(ツ)_/¯)\n",
    "    for idy in range(len(labs)):\n",
    "        tmean  = tmean + len(posits[idy])*np.mean(data[posits[idy]],axis=0)\n",
    "    tmean = tmean/len(data)\n",
    "    for idy in range(len(labs)):\n",
    "        cmean = np.mean(data[posits[idy]],axis=0)\n",
    "        Sb = Sb + float(len(posits[idy]))*np.outer(cmean-tmean,cmean-tmean)\n",
    "    return Sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  63.21213333,  -19.534     ,  165.16466667,   71.36306667],\n",
       "       [ -19.534     ,   10.9776    ,  -56.0552    ,  -22.4924    ],\n",
       "       [ 165.16466667,  -56.0552    ,  436.64373333,  186.90813333],\n",
       "       [  71.36306667,  -22.4924    ,  186.90813333,   80.60413333]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sBetween(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And $S_T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sTotal(data, target):\n",
    "    tmean = np.mean(data,axis=0)\n",
    "    St = np.zeros([len(data[0]),len(data[0])])\n",
    "    for vec in data:\n",
    "        St = St + np.outer(vec-tmean,vec-tmean)\n",
    "    return St"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 102.16833333,   -5.851     ,  189.77866667,   77.01866667],\n",
       "       [  -5.851     ,   28.0126    ,  -47.9352    ,  -17.5792    ],\n",
       "       [ 189.77866667,  -47.9352    ,  463.86373333,  193.16173333],\n",
       "       [  77.01866667,  -17.5792    ,  193.16173333,   86.77973333]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sTotal(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate scalars for the scatter criteria using:\n",
    "* Trace criterion: calculating the sum of the diagonal of any matrix\n",
    "* Determinant criterion: calculating the determinant of the matrix (does not apply if the number of clusters is less than the number dimensions)\n",
    "* Invarian criterion: Based on the eigenvalues ($\\lambda_i$) of the linear transformers (matrixes).\n",
    " 1. $tr[S_{W}^{-1}S_B]=\\sum\\limits_{i=1}^d \\lambda_i$\n",
    " 2. $tr[S_{T}^{-1}S_W] = \\sum\\limits_{i=1}^d \\frac{1}{1+\\lambda_i}$\n",
    " 3. $\\frac{\\mid S_w \\mid}{\\mid S_T \\mid}=\\prod\\limits_{i=1}^{d} \\frac{1}{1+\\lambda_i}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace Sw, Sb and St =  89.3868 591.4376 680.8244\n",
      "Determinant Sw, Sb and St =  22069.1091687 5.5189899733e-27 938094.951014\n",
      "Trace Sw^-1Sb =  32.5495246636\n",
      "Trace St^-1Sw =  2.81279323666\n",
      "|Sw|/|St| =  0.106313798265\n"
     ]
    }
   ],
   "source": [
    "print(\"Trace Sw, Sb and St = \",np.trace(sWithin(iris.data,iris.target)),np.trace(sBetween(iris.data,iris.target)),\\\n",
    "     np.trace(sTotal(iris.data,iris.target)))\n",
    "print(\"Determinant Sw, Sb and St = \",np.linalg.det(sWithin(iris.data,iris.target)),\\\n",
    "      np.linalg.det(sBetween(iris.data,iris.target)),\\\n",
    "      np.linalg.det(sTotal(iris.data,iris.target)))\n",
    "print(\"Trace Sw^-1Sb = \",np.trace(np.matmul(np.linalg.inv(sWithin(iris.data,iris.target)),sBetween(iris.data,iris.target))))\n",
    "print(\"Trace St^-1Sw = \",np.trace(np.matmul(np.linalg.inv(sTotal(iris.data,iris.target)),sWithin(iris.data,iris.target))))\n",
    "print(\"|Sw|/|St| = \",np.linalg.norm(sWithin(iris.data,iris.target))/np.linalg.norm(sTotal(iris.data,iris.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-Criterion\n",
    "We will skip the condorcet's criterion and go straight to the C-Criterion. The C-Criterion can be stated in terms of a similarity objective function to be maximized:\n",
    "$$OF = \\sum_{C_i \\in C} \\sum_{\\substack{x_j,x_k \\in C_i \\\\ x_j \\neq X_k}}(s(x_j,x_k)-\\gamma) + \\sum_{C_i \\in C} \\sum_{\\substack{x_j \\in C \\\\ x_k \\notin C_i}}(\\gamma - s(x_j,x_k))$$\n",
    "However, since we are dealing with numerical vectors, we will use a distance objective function to be minimzed:\n",
    "$$OF = \\sum_{C_i \\in C} \\sum_{\\substack{x_j,x_k \\in C_i \\\\ x_j \\neq X_k}}(d(x_j,x_k)-\\gamma) + \\sum_{C_i \\in C} \\sum_{\\substack{x_j \\in C \\\\ x_k \\notin C_i}}(\\gamma - d(x_j,x_k))$$\n",
    "Here, $\\gamma$ is an arbitrary treshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28426.62094691259"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cCriterion(data,target,gamma=0,dissimilarity=spd.euclidean):\n",
    "    from scipy.spatial import distance as spd\n",
    "    labs = getLabs(data,target)\n",
    "    posits = getPosits(data,target)\n",
    "    s1 = 0\n",
    "    s2 = 0\n",
    "    # Calculating the first sum\n",
    "    for idy in range(len(labs)):\n",
    "        for idj in range(0,(len(posits[idy])-1)):\n",
    "            for idk in range((idj+1),len(posits[idy])):\n",
    "                s1 = s1 + dissimilarity(data[posits[idy][idj]],data[posits[idy][idk]])-gamma\n",
    "    # calculating the second sum (yeah, four nested for loops, doesn't seem like an efficient metric)\n",
    "    for idy in range((len(labs)-1)):\n",
    "        for idz in range((idy+1),len(labs)):\n",
    "            for idj in range(len(posits[idy])):\n",
    "                for idk in range(len(posits[idz])):\n",
    "                    s2 = s2 - gamma + dissimilarity(data[posits[idy][idj]],data[posits[idz][idk]])\n",
    "    res = s1 + s2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28426.62094691259"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cCriterion(iris.data,iris.target,gamma=0,dissimilarity=spd.euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forewords\n",
    "We stop our quality criteria analisys here. We won't cover the subject of External Qualoty Criteria because this type of procedure will be better developed in the Supervised Learning notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
